<!DOCTYPE html>
<html lang="en-US">
    <head>
        <meta charset="UTF-8">
        <!-- Begin Jekyll SEO tag v2.8.0 -->
        <title>Birds? Kaggle Competition</title>
        <meta name="generator" content="Jekyll v3.9.3"/>
        <meta property="og:title" content="Birds? Kaggle Competition"/>
        <meta property="og:locale" content="en_US"/>
        <meta name="description" content="A Kaggle Image Classification Competition as CSE 455 Final"/>
        <meta property="og:description" content="A Kaggle Image Classification Competition as CSE 455 Final"/>
        <link rel="canonical" href="https://calebhuangsea.github.io./bird/"/>
        <meta property="og:url" content="https://calebhuangsea.github.io./bird/"/>
        <meta property="og:site_name" content="Birds? Kaggle Competition"/>
        <meta property="og:type" content="website"/>
        <meta name="twitter:card" content="summary"/>
        <meta property="twitter:title" content="Birds? Kaggle Competition"/>
        <script type="application/ld+json">
            
{"@context":"https://schema.org","@type":"WebSite","description":"A Kaggle Image Classification Competition as CSE 455 Final","headline":"Birds? Kaggle Competition","name":"Birds? Kaggle Competition","url":"https://calebhuangsea.github.io./bird/"}
        </script>
        <!-- End Jekyll SEO tag -->
        <link rel="preconnect" href="https://fonts.gstatic.com">
        <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#157878">
        <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
        <link rel="stylesheet" href="styles.css">
        <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->
        <!-- Setup Google Analytics -->
        <!-- You can set your favicon here -->
        <!-- link rel="shortcut icon" type="image/x-icon" href="/cse455_final/favicon.ico" -->
        <!-- end custom head snippets -->
    </head>
    <body>
        <a href="code.html" class="floating-button">Code</a>
        <a id="skip-to-content" href="#content">Skip to the content.</a>
        <header class="page-header" role="banner">
            <a class="redirect-button" href="https://docs.google.com/document/d/16AlA4xTmxlrb2tmAbytKBynr_YmsjrtlMJKzhXaTI6k/edit?usp=sharing">Report</a>
            <h1 class="project-name">Birds? Kaggle Competition</h1>
            
            <h2 class="project-tagline">A Kaggle Image Classification Competition as CSE 455 Final</h2>
            <h2 class="project-tagline">Team: Birdy Ensemblers <b><u>(Rank: 1 / 26)</u></b></h2>
            <h3 class="project-tagline2"><a href="https://github.com/hqfang44">Haoquan Fang</a></h2>
            <h3 class="project-tagline2"><a href="https://github.com/QianchengLi2333">Qiancheng Li</a></h2>
            <h3 class="project-tagline2"><a href="https://github.com/zihaohugh">Zihao Wang</a></h2>
            <h3 class="project-tagline2"><a href="https://github.com/calebhuangsea">Cile Huang</a></h2>
            <a href="https://github.com/Birdy-Ensemblers/Birds" class="btn">View on GitHub</a>
        </header>
        <main id="content" class="main-content" role="main">
            <p>For this competition, we are required to solve an image classification task to identify bird species.</p>
            <h1 id="dataset">Dataset</h1>
            <p>
                There are <strong>555</strong>
                classes, approximately <strong>38,000</strong>
                images of varying sizes in the <strong>training</strong>
                set, and <strong>10,000</strong>
                in the <strong>testing</strong>
                set.
            </p>
            <p>Below is some sample data.</p>
            <p>
                <img src="sample.png" alt="sample_data"/>
            </p>
            <p>This histogram of images per class is presented here.</p>
            <p>
                <img src="distribution.png" alt="histogram"/>
            </p>
            <p>total images: 38562</p>
            <p>min class: Dark-eyed Junco (White-winged), num: 10</p>
            <p>max class: White-winged Dove, num: 106</p>
            <p>standard deviation: 22.06846640893006</p>
            <p>Notice that the classes distribution here is uneven. The class with the most instances is ten times the class with the least instances. The standard deviation is also very big.</p>
            <h1 id="balancing-the-data">Balancing the data</h1>
            <p>We have explored two ways to balance the number of instances for each class.</p>
            <h2>(1) Data augmentation:</h2>
            <p>Data augmentation is a technique for artificially increasing the number of samples in the training dataset in order to prevent the model from overfitting. Transformations that are applied should maintain the data’s coherence with the distribution from which it was drawn, as extreme parameters may impede the model’s ability to learn.</p>
            <p>In order to optimize the image loading process, we have resized all the images to 224x224 and stored them on an SSD as the new dataset.</p>
            <p>
                <img src="aug.png" alt="Augmentation"/>
            </p>
            <p>We used <b>Resized, Random Crop, Random Horizontal Flip, Random Affine, Random Rotation, Gaussian Blur, Random Gray Scale</b> as transformations for train data.</p>
            <p>And we only used <b>Resized, Center Crop</b> as transformations for test data.</p>
            <h2>(2) Class weights:</h2>
            <p>We realize in the train dataset, some classes only have little amount of data comparing to other classes. If we trained the model without any preprocessing, the model will have poor performance on these minority classes.</p>
            <p>So we used <b>Weighted Random Sampler</b> on the train data to solve this problem, where we assign classes that have more input data with a smaller weight, and assign minority classes with higher weight.</p>
            <p>
                <img src="weight.png" alt="Weighted Random Sampler"/>
            </p>
            <h2 id="Train/Validation/Test Set Splitting">Train / Validation / Test Set Splitting</h2>
            <p>Before we dive into the training process, we first split our dataset into two categories: training set, validation set, and test set. In this contest, we were given two folders of data. One is the ground truth dataset and one is the dataset with unknown labels. Hence, we split the first dataset into a training set and a validation set with a ratio of 8 : 2, and the second dataset automatically becomes the test set, where the accuracy is measured after we submit our prediction to the competition website. By the way, image augmentation transforms are only applied to the training set.</p>
            <h2 id="Training">Training</h2>
            <p>During training, we use CrossEntropy as the loss function as the principal performance metric. We employ SGD with custom weight decay (0.0005) and momentum (0.9) as the optimizer. We use the biggest batch size possible as long as we have enough GPU memories, and we use a batch size of 64 for smaller models and 32 for bigger ones. We also make use of the learning rate scheduler and experimented with the random weight sampler. However, the random weight sampler doesn’t improve the performance, so we didn’t use it in our final pipeline.</p>
            <h2 id="Models and Ensemble">Models and Ensemble</h2>
            <p>We have been experimenting with various CV attention models, including MaxViT, ConvNeXt, and Swin Transformer V2. For the model size, we choose the tiny version for MaxViT, the small version for ConvNeXt, and the tiny version for Swin Transformer V2. Among the three models, MaxViT performs the best, which reaches an 83.65% accuracy in the test set. Then is ConvNeXt, which reaches an 81.85% accuracy in the test set. Finally is Swin Transformer V2, which reaches an 81.55% accuracy in the test set. In our final submission, we also perform model ensemble to integrate all three models together to get a better performance. We do grid search to find the best coefficients between the three models so that they have the highest accuracy on the validation set using the coefficients. We finally choose the coefficient between MaxViT, ConvNeXt, and Swin Transformer V2 to be 0.39 : 0.26 : 0.35.</p>
            <h2 id="Final Result">Final Result</h2>
            <p>The final pipeline achieved a score of 88% on the test set.</p>
            <p>
                <a href="">Video Presentation</a>
            </p>
        </main>
    </body>
</html>
